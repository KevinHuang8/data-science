{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will be building a basic neural network from scratch, consisting of only fully connected (dense) layers.\n",
    "\n",
    "We begin by defining the *Layer* class, which represents a fully connected layer in the NN. Each layer has a set of associated weights and biases, as well as an activation function and its derivative (which defaults to the relu). The weights and biases are initialized to random small numbers (0 - 0.1). The layer stores two values, *X* and *delta*, which are the output of that layer from the forwards and backwards pass, respectively. \n",
    "\n",
    "At first, we will handle the forward pass, which computes *X*. When computing forward, we simply multiply the input by the weight matrix and add the bias, and then apply the activation function. The layer then stores the output as *X*, and also stores the output before applying the activation function, as that is needed in backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The rectified linear unit\n",
    "def relu(input):\n",
    "    return np.maximum(0, input)\n",
    "\n",
    "def relu_derivative(input):\n",
    "    # If a element >= 0, set it equal to 1, otherwise 0.\n",
    "    # This is the fastest method, see https://stackoverflow.com/questions/45648668/convert-numpy-array-to-0-or-1 \n",
    "\n",
    "    return (input >= 0).astype(int)\n",
    "\n",
    "# Represents a fully connected layer in the neural network\n",
    "class Layer:\n",
    "    def __init__(self, output_size, input_size, activation=None, activation_derivative=None):\n",
    "        self.output_size = output_size\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.weights = np.random.rand(output_size, input_size) * 0.1\n",
    "        self.bias = np.random.rand(output_size, 1) * 0.1\n",
    "        \n",
    "        # Default activation function is relu\n",
    "        if activation is None:\n",
    "            self.activation = relu\n",
    "            self.activation_derivative = relu_derivative\n",
    "        else:\n",
    "            self.activation = activation\n",
    "            self.activation_derivative = activation_derivative\n",
    "        \n",
    "    def forward(self, input):\n",
    "        assert input.shape == (input_size, 1)\n",
    "        \n",
    "        signal = np.dot(self.weights, input) + np.bias\n",
    "        \n",
    "        output = self.activation(signal)\n",
    "        \n",
    "        assert output.shape == (output_size, 1)    \n",
    "        \n",
    "        # We save the signal for use in backpropagation\n",
    "        self.signal = signal\n",
    "        \n",
    "        self.X = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we handle the backwards pass. The backward functions calculates *delta* of the current layer, given the layer ahead of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, input_layer):\n",
    "    input = input_layer.delta\n",
    "    weights = input_layer.weights\n",
    "    \n",
    "    assert input.shape == (input_layer.output_size, 1)\n",
    "    \n",
    "    output = np.dot(weights.T, input)\n",
    "    output *= self.activation_derivative(self.signal)\n",
    "    \n",
    "    assert output.shape == (output_size, 1)\n",
    "    \n",
    "    self.delta = output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
